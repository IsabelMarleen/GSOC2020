#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jun 30 17:12:23 2020

@author: isapoetzsch
"""


import requests
import json
from pandas.io.json import json_normalize

instance = "https://synbiohub.org/"

fl = open("query_select_collection.py", "r")
sparqlquery = fl.read()
sparqlquery = sparqlquery.replace("OFFSET 0", "OFFSET 1")
        

r = requests.post(instance+"sparql", data = {"query":sparqlquery}, headers = {"Accept":"application/json"})

d = json.loads(r.text)
a = json_normalize(d['results']['bindings'])

def sparqling(query_text, is_basic = True, 
              no_sequence = False, progress = True):
    all_pages = []
    
    #loops over all pages and extracts query results
    for i in range(0,2000):
        
        if progress: #print progress
            print(i)
            
        #replace placeholder in query_text with page number to get
        queryed = query_text.replace("replacehere", str(i*10000))
        
        #make request for data
        r = requests.post("https://synbiohub.org/sparql",
                          data = {"query":queryed},
                          headers = {"Accept":"application/json"})
        
        #reformat page data
        d = json.loads(r.text)
        one_page = json_normalize(d['results']['bindings'])
        
        #add page data to all pages data
        all_pages.append(one_page)
        
        #if the page was no longer a full page stop loop
        if len(one_page)<10000:
            break
        
    #create pandas data frame containing all page info



